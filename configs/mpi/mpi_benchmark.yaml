# MPI Benchmark Configuration
# ===========================
# Purpose: Performance benchmarking and scaling tests
# Expected runtime: Variable (depends on benchmark settings)
# Usage: mpirun -n <N> python scripts/run_mps_mpi.py --config configs/mpi/mpi_benchmark.yaml --benchmark

extends: ../default.yaml

network:
  # Scalable network size
  # Override with: --override network.n_sensors=200
  n_sensors: ${MPS_N_SENSORS:50}
  n_anchors: eval: int(0.15 * 50)  # 15% anchors
  scale: 20.0
  communication_range: 0.25
  topology: "grid"  # Grid for consistent benchmarking

measurements:
  # Minimal noise for benchmarking
  noise_factor: 0.01
  outlier_probability: 0.0
  seed: 12345  # Different seed for benchmarking

algorithm:
  # Optimized for speed
  gamma: 0.999
  alpha: 20.0  # Aggressive for benchmarking
  max_iterations: 300  # Fixed iterations for fair comparison
  tolerance: 1e-8  # Won't trigger early stop
  early_stopping: false  # Disable for consistent timing
  verbose: false  # Minimal output for benchmarking

admm:
  iterations: 30  # Reduced for speed
  tolerance: 1e-4
  rho: 2.0  # Higher for faster convergence
  warm_start: true

mpi:
  enable: true
  async_communication: true
  buffer_size_kb: 4096  # Large buffer for performance
  collective_operations: true
  checkpoint_interval: 0  # No checkpoints during benchmark
  load_balancing: "cyclic"  # Better for benchmarking

performance:
  track_metrics: true
  log_interval: 0  # No logging during benchmark
  save_checkpoints: false

output:
  save_results: true
  output_dir: "results/benchmarks/"
  save_interval: 0  # No intermediate saves
  save_positions: false  # Only timing matters
  save_metrics: true
  plot_results: false
  verbose: false