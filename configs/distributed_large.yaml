# ============================================================================
# DISTRIBUTED LARGE-SCALE CONFIGURATION
# ============================================================================
# Purpose: MPI-based distributed execution for large sensor networks
# Use Case: Large-scale deployments requiring parallel processing
#           - Smart cities (1000s of sensors)
#           - Environmental monitoring networks
#           - Large indoor facilities (airports, warehouses)
#           - Military/defense applications
# Expected Performance:
#   - Runtime: 20-60 seconds (depends on MPI processes)
#   - Relative Error: 0.15-0.25
#   - RMSE: 1-3 meters (in 50m × 50m area)
#   - Speedup: ~0.7 × N_processes (sublinear due to communication)
# Requirements:
#   - MPI installation (OpenMPI, MPICH, etc.)
#   - Run with: mpirun -n <processes> python scripts/run_distributed.py
# Scaling Guidelines:
#   - 2-4 processes: 50-100 sensors
#   - 4-8 processes: 100-200 sensors
#   - 8-16 processes: 200-500 sensors
#   - 16+ processes: 500+ sensors
# ============================================================================

# ----------------------------------------------------------------------------
# NETWORK CONFIGURATION - Large-scale deployment
# ----------------------------------------------------------------------------
network:
  # Large network suitable for distributed processing
  # 100 sensors is where MPI starts showing benefits
  # Single-process would take 60+ seconds
  # With 8 processes: ~15 seconds
  n_sensors: 100
  
  # 10% anchors provides good coverage for large area
  # More anchors would be expensive for large deployment
  # Strategic placement at area boundaries recommended
  n_anchors: 10
  
  # Large deployment area: 50m × 50m (2500 m²)
  # Realistic for:
  #   - Building floor (office, warehouse)
  #   - Outdoor area (parking lot, plaza)
  #   - Agricultural field section
  scale: 50.0
  
  # 2D localization (most common for large areas)
  # 3D would require more anchors and computation
  dimension: 2
  
  # Moderate communication range for realistic connectivity
  # 0.25 × 50m = 12.5m communication radius
  # Typical for:
  #   - WiFi: 10-30m indoors
  #   - Bluetooth 5: 10-40m
  #   - ZigBee: 10-100m
  # Average degree: ~π × (0.25)² × 100 ≈ 20 neighbors
  communication_range: 0.25
  
  # Random topology models real deployments
  # "grid": For systematic coverage (less realistic)
  # "cluster": For grouped deployments (e.g., room-based)
  topology: "random"

# ----------------------------------------------------------------------------
# MEASUREMENT CONFIGURATION - Realistic large-scale conditions
# ----------------------------------------------------------------------------
measurements:
  # Standard distance measurements
  # Most large-scale systems use TOA/TDOA
  # "rssi": For WiFi/BLE (noisier)
  # "toa": For UWB/acoustic
  # "tdoa": For cellular/GPS
  measurement_type: "distance"
  
  # Realistic noise for large-scale deployment
  # 3% models good conditions with quality hardware
  # Large areas have more environmental variation
  # Temperature gradients, humidity changes affect ranging
  noise_factor: 0.03
  
  # Moderate outlier rate for robustness testing
  # 5% outliers from:
  #   - Occasional NLOS paths
  #   - Interference from other systems
  #   - Environmental obstacles (people, vehicles)
  outlier_probability: 0.05
  
  # Fixed seed for reproducible testing
  # Change for different network realizations
  seed: 42

# ----------------------------------------------------------------------------
# ALGORITHM CONFIGURATION - Tuned for distributed execution
# ----------------------------------------------------------------------------
algorithm:
  # Distributed MPS variant
  # "distributed_mps": Our MPI implementation
  # "consensus_admm": Alternative distributed method
  name: "distributed_mps"
  
  # Conservative gamma for distributed stability
  # 0.995 slightly lower than single-process (0.999)
  # Accounts for:
  #   - Communication delays
  #   - Partial information at each process
  #   - Numerical differences across processes
  gamma: 0.995
  
  # Under-relaxed alpha for distributed convergence
  # α=0.8 < 1.0 ensures stability
  # Distributed algorithms need conservative steps
  # Prevents divergence from inconsistent updates
  # Trade-off: Slower convergence for guaranteed stability
  alpha: 0.8
  
  # More iterations for distributed convergence
  # Distributed typically needs 1.5-2× more iterations
  # Due to:
  #   - Information propagation delays
  #   - Consensus building across processes
  #   - Conservative parameters
  max_iterations: 1000
  
  # Tight tolerance for high-quality solution
  # 1e-5 relative = ~0.5mm in 50m area
  # May not achieve but provides good target
  # Distributed can match single-process accuracy
  tolerance: 1e-5

# ----------------------------------------------------------------------------
# MPI CONFIGURATION - Distributed execution settings
# ----------------------------------------------------------------------------
mpi:
  # MUST BE TRUE for this configuration
  # Without MPI, falls back to single-process (slow)
  enable: true
  
  # Asynchronous communication for better performance
  # Non-blocking sends/receives allow computation overlap
  # Pros:
  #   - Better CPU utilization
  #   - Hides communication latency
  #   - 20-30% faster overall
  # Cons:
  #   - More complex implementation
  #   - Potential race conditions
  #   - May affect convergence slightly
  async_communication: true
  
  # MPI buffer size (missing _kb suffix in original)
  # 1024 KB = 1 MB buffer per process
  # Holds: ~128K double values
  # Sufficient for 100 sensors with 20 neighbors each
  # Increase for larger networks: 2048-4096 KB
  buffer_size: 1024
  
  # Use tree-based collective operations
  # AllReduce for consensus updates (O(log P) vs O(P))
  # Broadcast for network data distribution
  # Much more efficient than point-to-point
  # Essential for scalability beyond 8 processes
  collective_operations: true

# ----------------------------------------------------------------------------
# PERFORMANCE CONFIGURATION - Monitoring and fault tolerance
# ----------------------------------------------------------------------------
performance:
  # Track detailed metrics for analysis
  # Essential for distributed debugging
  # Shows load imbalance, communication bottlenecks
  track_metrics: true
  
  # Regular progress updates
  # Every 50 iterations = 5% progress
  # Important for long distributed runs
  # Helps identify stuck processes
  log_interval: 50
  
  # Checkpoint for fault tolerance
  # Save state every 100 iterations
  # Allows restart if process fails
  # Critical for large clusters
  # Overhead: ~1-2% of runtime
  checkpoint_interval: 100

# ----------------------------------------------------------------------------
# OUTPUT CONFIGURATION - Distributed results handling
# ----------------------------------------------------------------------------
output:
  # Save results (rank 0 only)
  # Only master process writes files
  # Prevents file conflicts
  save_results: true
  
  # Dedicated directory for distributed runs
  # Separate from single-process results
  # Easier to compare performance
  output_dir: "results/distributed/"
  
  # Enable checkpointing for restart capability
  # Creates .ckpt files with full state
  # Can resume from last checkpoint
  save_checkpoints: true
  
  # Verbose output for distributed debugging
  # Shows per-process statistics
  # Warning: Can be very noisy with many processes
  # Consider false for production runs
  # Use log files instead: mpirun -n 8 ... > log.txt
  verbose: true