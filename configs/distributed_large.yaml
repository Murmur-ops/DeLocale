# Distributed Large-Scale Configuration
# =====================================
# Purpose: MPI-based distributed execution for large networks
# Expected runtime: 20-60 seconds (depends on MPI processes)
# Expected RMSE: 1-3 meters (in 50m x 50m area)
# Required: Run with mpirun -n <processes> python scripts/run_distributed.py

network:
  # Large network for distributed testing
  n_sensors: 100
  
  # 10% anchors is good balance for large networks
  n_anchors: 10
  
  # Network scale in meters (50m x 50m area)
  # Larger area = more realistic deployment scenario
  scale: 50.0
  
  # 2D localization
  dimension: 2
  
  # Communication range as fraction of scale
  # 0.25 * 50m = 12.5m communication radius
  # Creates moderately connected network
  communication_range: 0.25
  
  # Network topology generation method
  # Options: "random", "grid", "cluster"
  topology: "random"

measurements:
  # Type of distance measurement
  # Options: "distance", "rssi", "toa", "tdoa"
  measurement_type: "distance"
  
  # Measurement noise (3% is realistic for good conditions)
  # Applied as: measured = true * (1 + noise_factor * random)
  noise_factor: 0.03
  
  # Probability of outlier measurements
  # 5% outliers tests robustness
  outlier_probability: 0.05
  
  # Random seed for reproducibility
  seed: 42

algorithm:
  # Algorithm variant for distributed execution
  # Options: "distributed_mps", "consensus_admm"
  name: "distributed_mps"
  
  # Proximal gradient parameter
  # 0.995 = conservative for stability in distributed setting
  gamma: 0.995
  
  # Step size / over-relaxation
  # 0.8 = under-relaxed for distributed convergence
  alpha: 0.8
  
  # Maximum iterations
  # Distributed typically needs more iterations
  max_iterations: 1000
  
  # Convergence tolerance in meters
  # 1e-5 = 10 micrometers (very tight)
  tolerance: 1e-5

mpi:
  # Enable MPI distributed execution
  # REQUIRED for this configuration
  enable: true
  
  # Use asynchronous communication
  # Faster but may affect convergence
  async_communication: true
  
  # MPI buffer size in KB
  buffer_size: 1024
  
  # Use collective operations (AllReduce, etc.)
  # More efficient for global computations
  collective_operations: true

performance:
  # Track detailed performance metrics
  track_metrics: true
  
  # Log progress every N iterations
  log_interval: 50
  
  # Save checkpoint every N iterations
  # Useful for long runs and fault tolerance
  checkpoint_interval: 100

output:
  # Save final results to JSON
  save_results: true
  
  # Output directory
  output_dir: "results/distributed/"
  
  # Save intermediate checkpoints
  save_checkpoints: true
  
  # Verbose output (may be noisy with MPI)
  # Consider setting false for production runs
  verbose: true